{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1kNi8n4TLfZYqXD5IvnRjH5FvGf39NtvB","timestamp":1678745410532},{"file_id":"1lDwKZYjOaAeP1OIhEqVyQKIKVAArIPFQ","timestamp":1677601006972}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install dgl"],"metadata":{"id":"vwzoA7pc7Ru1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678877437610,"user_tz":-330,"elapsed":4373,"user":{"displayName":"Sanchit Sinha","userId":"03766366414454501043"}},"outputId":"7dafa8c4-ae35-46b4-aac5-c2a7df6c219c"},"execution_count":263,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: dgl in /usr/local/lib/python3.9/dist-packages (1.0.1)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.9/dist-packages (from dgl) (1.22.4)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from dgl) (1.10.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from dgl) (4.65.0)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.9/dist-packages (from dgl) (5.9.4)\n","Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.9/dist-packages (from dgl) (3.0)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from dgl) (2.25.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->dgl) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->dgl) (2022.12.7)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->dgl) (4.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->dgl) (2.10)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"oJyd7FY6_cFa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dependencies"],"metadata":{"id":"wjNRn5fB_ctw"}},{"cell_type":"code","source":["import torch\n","import dgl\n","from collections import namedtuple\n","import dgl.function as fn\n","from copy import deepcopy as dc\n","import random\n","import time\n","from time import time\n","from torch.utils.data import DataLoader\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from dgl import DGLGraph"],"metadata":{"id":"VJ4QvIIF7xbb","executionInfo":{"status":"ok","timestamp":1678877451126,"user_tz":-330,"elapsed":3,"user":{"displayName":"Sanchit Sinha","userId":"03766366414454501043"}}},"execution_count":264,"outputs":[]},{"cell_type":"markdown","source":["## Graph"],"metadata":{"id":"e-nPNcXzHgmp"}},{"cell_type":"code","source":["cost = torch.tensor([300, 300, 300, 300, 300])\n","\n","features = torch.tensor([cost.cpu().detach().numpy()])\n","print(type(features), len(features), features)\n","print(type(features[0]), len(features[0]), features[0])\n","\n","maxb = 1000\n","src_ids = torch.tensor([0, 1, 2, 3])\n","dst_ids = torch.tensor([1, 2, 3, 4])\n","\n","g = dgl.graph((src_ids, dst_ids), num_nodes=5)\n","g = dgl.add_self_loop(g)\n","g.ndata['feat'] = torch.tensor([[300, 0],[300, 0],[300, 0],[300, 0],[300, 0]])\n","\n","print(g.ndata)\n","print(g.nodes())\n","print(g.edges())\n","print(g.ndata['feat'].shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G3eW_PN7HfOy","executionInfo":{"status":"ok","timestamp":1678877452433,"user_tz":-330,"elapsed":19,"user":{"displayName":"Sanchit Sinha","userId":"03766366414454501043"}},"outputId":"1281440f-c02a-4ce1-dcc9-6a3a79fc8b73"},"execution_count":265,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'torch.Tensor'> 1 tensor([[300, 300, 300, 300, 300]])\n","<class 'torch.Tensor'> 5 tensor([300, 300, 300, 300, 300])\n","{'feat': tensor([[300,   0],\n","        [300,   0],\n","        [300,   0],\n","        [300,   0],\n","        [300,   0]])}\n","tensor([0, 1, 2, 3, 4])\n","(tensor([0, 1, 2, 3, 0, 1, 2, 3, 4]), tensor([1, 2, 3, 4, 0, 1, 2, 3, 4]))\n","torch.Size([5, 2])\n"]}]},{"cell_type":"markdown","source":["## Model\n","https://discuss.dgl.ai/t/node-regression-in-heterograph/2492/6"],"metadata":{"id":"OYWBA7fY3KyE"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from dgl.nn import GraphConv\n","\n","# class RGCN(nn.Module):\n","#     def __init__(self, in_feats, h_feats, num_classes, seed):\n","#         super(RGCN, self).__init__()\n","#         self.seed = torch.manual_seed(seed)\n","#         self.conv1 = GraphConv(in_feats, h_feats)\n","#         self.conv2 = GraphConv(h_feats, h_feats)\n","#         self.lin = nn.Linear(h_feats, num_classes)\n","    \n","#     # def forward(self, g, in_feat):\n","#     #     h = self.conv1(g, in_feat)\n","#     #     h = F.relu(h)\n","#     #     h = self.conv2(g, h)\n","#     #     h = F.relu(h)\n","#     #     h = self.lin(h)\n","#     #     return h\n","    \n","#     def forward(self, g, in_feat):\n","#         x1 = F.relu(self.conv1(g, in_feat)) #ReLU activation function\n","#         x2 = F.relu(self.conv2(g, x1))\n","#         return self.lin(x2)\n","\n","# model = RGCN(g.ndata['feat'].shape[1], 16, dataset.num_classes)\n","# 2 16 action_space\n","\n","\n","class RGCN(nn.Module):\n","    def __init__(self, in_feats, h_feats, num_classes, seed):\n","        super(RGCN, self).__init__()\n","        self.seed = seed\n","        self.conv1 = GraphConv(in_feats, h_feats)\n","        self.conv2 = GraphConv(h_feats, h_feats)\n","        self.fc3 = nn.Linear(h_feats, num_classes)\n","        self.num_classes = num_classes\n","\n","    def forward(self, g, in_feat):\n","        inp = len(in_feat) \n","        print(\"0\", inp)\n","        h = self.conv1(g, in_feat)\n","        print(\"1\", h, h.shape)\n","        h = F.relu(h)\n","        print(\"2\", h, h.shape)\n","        h = self.conv2(g, h)\n","        print(\"3\", h, h.shape)\n","        h = self.fc3(F.relu(h))\n","        print(\"4\", h, h.shape)\n","        T = h.mean(dim=0)\n","        T = T.mean(dim=0)\n","        return T.reshape(inp, self.num_classes)\n"],"metadata":{"id":"j3zPG8vXUWhQ","executionInfo":{"status":"ok","timestamp":1678878685869,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sanchit Sinha","userId":"03766366414454501043"}}},"execution_count":293,"outputs":[]},{"cell_type":"code","source":["# model = RGCN(g.ndata['feat'].shape[1], 16, 5, 0)\n","\n","st1 = torch.tensor([1, 0, 0, 0, 0])\n","st2 = torch.tensor([1, 1, 0, 0, 0])\n","\n","f1 = g.ndata['feat']\n","f1[:, 1] = st1\n","\n","f2 = g.ndata['feat']\n","f2[:, 1] = st2\n","\n","# model = RGCN(f1.shape[1], 16, 5, 0)\n","# print(st1, f1, f1.shape)\n","# action = model(g, f1)\n","# print(action)\n","\n","# # f = torch.tensor()\n","\n","\n","f2 = torch.tensor([[[300,   1],\n","        [300,   1],\n","        [300,   0],\n","        [300,   0],\n","        [300,   0]]])\n","\n","f1 = np.array([[300,   1],\n","        [300,   1],\n","        [300,   0],\n","        [300,   0],\n","        [300,   0]])\n","f3 = torch.tensor([f1 ,f1])\n","print(f3, f3.shape)\n","# print(st2, f2, f2.shape)\n","model = RGCN(f3.shape[2], 16, 5, 0)\n","action = model(g, f3)\n","print(action)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":572},"id":"_p0sZJZN4xbq","executionInfo":{"status":"error","timestamp":1678878751695,"user_tz":-330,"elapsed":965,"user":{"displayName":"Sanchit Sinha","userId":"03766366414454501043"}},"outputId":"5f180bbe-f8e8-484e-c9fd-d5b8a3d5e037"},"execution_count":297,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[300,   1],\n","         [300,   1],\n","         [300,   0],\n","         [300,   0],\n","         [300,   0]],\n","\n","        [[300,   1],\n","         [300,   1],\n","         [300,   0],\n","         [300,   0],\n","         [300,   0]]]) torch.Size([2, 5, 2])\n","0 2\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-297-df1822bde356>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# print(st2, f2, f2.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRGCN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-293-11c902ffb589>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, g, in_feat)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/dgl/nn/pytorch/conv/graphconv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, graph, feat, weight, edge_weight)\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0mshp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfeat_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m                 \u001b[0mfeat_src\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeat_src\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (5) at non-singleton dimension 0"]}]},{"cell_type":"markdown","source":["## Agent"],"metadata":{"id":"ad16n6GO3PG7"}},{"cell_type":"code","source":["import numpy as np\n","import random\n","from collections import namedtuple, deque\n","\n","import torch\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","BUFFER_SIZE = int(1e5)  # replay buffer size\n","# BATCH_SIZE = 64         # minibatch size\n","BATCH_SIZE = 2         # minibatch size\n","GAMMA = 0.99            # discount factor\n","TAU = 1e-3              # for soft update of target parameters\n","LR = 5e-4               # learning rate \n","# UPDATE_EVERY = 4        # how often to update the network\n","UPDATE_EVERY = 1       # how often to update the network\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","class Agent():\n","    \"\"\"Interacts with and learns from the environment.\"\"\"\n","\n","    def __init__(self, state_size, action_size, seed):\n","        \"\"\"Initialize an Agent object.\n","        \n","        Params\n","        ======\n","            state_size (int): dimension of each state\n","            action_size (int): dimension of each action\n","            seed (int): random seed\n","        \"\"\"\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        print(g, features)\n","        self.seed = random.seed(seed)\n","\n","        # Q-Network\n","        # model = RGCN(g.ndata['feat'].shape[1], 16, dataset.num_classes)\n","        self.qnetwork_local = RGCN(g.ndata['feat'].shape[1], 16, action_size, seed).to(device)\n","        self.qnetwork_target = RGCN(g.ndata['feat'].shape[1], 16, action_size, seed).to(device)\n","        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n","\n","        # Replay memory\n","        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n","        # Initialize time step (for updating every UPDATE_EVERY steps)\n","        self.t_step = 0\n","    \n","    def step(self, state, action, reward, next_state, done):\n","        # Save experience in replay memory\n","        # print(\"Add to memory : \", state, action, next_state, reward)\n","        self.memory.add(state, action, reward, next_state, done)\n","        \n","        # Learn every UPDATE_EVERY time steps.\n","        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n","        if self.t_step == 0:\n","            # If enough samples are available in memory, get random subset and learn\n","            if len(self.memory) > BATCH_SIZE:\n","                experiences = self.memory.sample()\n","                self.learn(experiences, GAMMA)\n","\n","    def act(self, env, state, eps=0.):\n","        \"\"\"Returns actions for given state as per current policy.\n","        \n","        Params\n","        ======\n","            state (array_like): current state\n","            eps (float): epsilon, for epsilon-greedy action selection\n","        \"\"\"\n","        node_state = state[: -1 ]\n","        valid_action = []\n","        for i in range(len(node_state)):\n","            if(node_state[i] == 0 and env.cost[i] <= env.max_budget):\n","                valid_action.append(True)\n","            else:\n","                valid_action.append(False)\n","\n","        valid_action_arg = []\n","        for i in range(len(valid_action)):\n","            if valid_action[i] == True:\n","                valid_action_arg.append(i)\n","\n","\n","        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n","        self.qnetwork_local.eval()\n","        with torch.no_grad():\n","            new_features = g.ndata['feat']\n","            # print(state, self.state_size,  state[:,: self.state_size])\n","            new_features[:, 1] = state[:,: self.state_size]\n","            action_values = self.qnetwork_local(g, new_features)\n","            # print(\"training local model\", state, action_values)\n","        self.qnetwork_local.train()\n","\n","        mn = -1e9\n","        for i in range(len(valid_action)):\n","            if(valid_action[i] == False):\n","                action_values[0][i] = mn\n","\n","        # print(\"action_values\", action_values)\n","        # print(\"valid_idx\", valid_action_arg)\n","        \n","        # Epsilon-greedy action selection\n","        if random.random() > eps:\n","            # picks action with maximum value attached to it \n","            choice = np.argmax(action_values.cpu().data.numpy())\n","            if choice == mn : \n","                return -1\n","\n","            # print(\"non greedy : \", choice)\n","            return choice\n","        else:\n","            # randomly pick any action from the values that are not already chosen \n","            # return random.choice(np.arange(self.action_size))\n","            if len(valid_action_arg) == 0 :\n","                return -1\n","                \n","            choice = random.choice(valid_action_arg)\n","            # print(\"greedy : \", choice)\n","            return choice\n","\n","    def learn(self, experiences, gamma):\n","        \"\"\"Update value parameters using given batch of experience tuples.\n","\n","        Params\n","        ======\n","            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n","            gamma (float): discount factor\n","        \"\"\"\n","        states, actions, rewards, next_states, dones = experiences\n","        print(\"states\", states)\n","        print(\"actions\", actions)\n","        print(\"rewards\", rewards)\n","        print(\"next_states\", next_states)\n","        print(\"dones\", dones)\n","              \n","\n","        # Get max predicted Q values (for next states) from target model\n","        # print(\"0 (next_states) : \", (next_states), type(next_states))\n","        # print(\"1 self.qnetwork_target(next_states) : \", self.qnetwork_target(next_states), print(type(self.qnetwork_target(next_states))))\n","        # print(\"2 self.qnetwork_target(next_states).detach() : \", self.qnetwork_target(next_states).detach())\n","        # print(\"2.5 self.qnetwork_target(next_states).detach().max() : \", self.qnetwork_target(next_states).detach().max())\n","        # print(\"3  self.qnetwork_target(next_states).detach().max(1) : \", self.qnetwork_target(next_states).detach().max(1))\n","        # print(\"4 self.qnetwork_target(next_states).detach().max(1)[0] : \", self.qnetwork_target(next_states).detach().max(1)[0])\n","        # print(\"5 self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1) : \", self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1))\n","        # Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n","\n","        # Q_targets_next = self.qnetwork_target(g, features).detach().max(1)[0].unsqueeze(1)\n","        # # Compute Q targets for current states \n","        # Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n","\n","        Q_targets = []\n","        for i in range(len(next_states)) :\n","            ns = next_states[i]\n","            f = g.ndata['feat']\n","            print(\"0\", f, ns, ns[: self.state_size])\n","            f[:, 1] = ns[: self.state_size]\n","            print(\"1\", f)\n","            q_target_next = self.qnetwork_target(g, f).detach().max(1)[0].unsqueeze(1)\n","            print(\"2\", q_target_next)\n","            q_target = rewards[i] + (gamma * q_target_next * (1 - dones[i]))\n","            Q_targets.append(q_target)\n","\n","        # Get expected Q values from local model\n","        # print(\"0 self.qnetwork_local(states)\", self.qnetwork_local(states))\n","        # print(\"1 self.qnetwork_local(states).gather(1, actions)\", self.qnetwork_local(states).gather(1, actions))\n","        \n","        # Q_expected = self.qnetwork_local(states).gather(1, actions)\n","\n","\n","        # Q_expected = self.qnetwork_local(g, features).gather(1, actions)\n","        Q_expected = []\n","        for i in range(len(states)):\n","            s = states[i]\n","            f = g.ndata['feat']\n","            f[:, 1] = s[: self.state_size]\n","            action_values = self.qnetwork_local(g, f)\n","            print(\"3\", action_values)\n","            choice = np.argmax(action_values.cpu().data.numpy())\n","            q_expected = action_values[:, choice]\n","            Q_expected.append(q_expected)\n","\n","\n","        Q_expected = np.array(Q_expected)\n","        Q_targets = np.array(Q_targets)\n","\n","        Q_expected = torch.from_numpy(Q_expected)\n","        Q_targets = torch.from_numpy(Q_targets)\n","        \n","        # Compute loss\n","        loss = F.mse_loss(Q_expected, Q_targets)\n","        # Minimize the loss\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","\n","        # ------------------- update target network ------------------- #\n","        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)                     \n","\n","    def soft_update(self, local_model, target_model, tau):\n","        \"\"\"Soft update model parameters.\n","        θ_target = τ*θ_local + (1 - τ)*θ_target\n","\n","        Params\n","        ======\n","            local_model (PyTorch model): weights will be copied from\n","            target_model (PyTorch model): weights will be copied to\n","            tau (float): interpolation parameter \n","        \"\"\"\n","        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n","            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n","\n","\n","class ReplayBuffer:\n","    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n","\n","    def __init__(self, action_size, buffer_size, batch_size, seed):\n","        \"\"\"Initialize a ReplayBuffer object.\n","\n","        Params\n","        ======\n","            action_size (int): dimension of each action\n","            buffer_size (int): maximum size of buffer\n","            batch_size (int): size of each training batch\n","            seed (int): random seed\n","        \"\"\"\n","        self.action_size = action_size\n","        self.memory = deque(maxlen=buffer_size)  \n","        self.batch_size = batch_size\n","        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n","        self.seed = random.seed(seed)\n","    \n","    def add(self, state, action, reward, next_state, done):\n","        \"\"\"Add a new experience to memory.\"\"\"\n","        e = self.experience(state, action, reward, next_state, done)\n","        self.memory.append(e)\n","    \n","    def sample(self):\n","        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n","        experiences = random.sample(self.memory, k=self.batch_size)\n","\n","        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n","        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n","        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n","        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n","        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n","  \n","        return (states, actions, rewards, next_states, dones)\n","\n","    def __len__(self):\n","        \"\"\"Return the current size of internal memory.\"\"\"\n","        return len(self.memory)"],"metadata":{"id":"H3b4yeDgoNak","executionInfo":{"status":"ok","timestamp":1678872592650,"user_tz":-330,"elapsed":602,"user":{"displayName":"Sanchit Sinha","userId":"03766366414454501043"}}},"execution_count":258,"outputs":[]},{"cell_type":"markdown","source":["## Environment"],"metadata":{"id":"aU9M6sGe_L-3"}},{"cell_type":"code","source":["import torch\n","import dgl\n","from collections import namedtuple\n","import dgl.function as fn\n","from copy import deepcopy as dc\n","import random\n","import time\n","from time import time\n","from torch.utils.data import DataLoader\n","import numpy as np\n","from collections import deque\n","\n","class IM(object):\n","    def __init__(self, max_budget, p, num_nodes, cost):\n","        self.max_budget = max_budget\n","        self.BUDGET = max_budget\n","        assert(p <= 1 and p >= 0)\n","        self.p = p\n","        self.num_nodes = num_nodes\n","        self.cost = cost\n","\n","    def compute_reward(self, state):\n","        # reward is the number of additional nodes influenced\n","        reward = 0\n","        # each node has one chance to influence each neighbour\n","        # print(state)\n","        new_influenced = state.detach().cpu().numpy().ravel()\n","        # print(new_influenced)\n","        tot_influenced = state.detach().cpu().numpy().ravel()\n","        while((new_influenced == 1).sum() >= 1):\n","            # next = torch.full(\n","            #     (self.num_nodes, 1),\n","            #     0, \n","            #     dtype = torch.long\n","            #     )\n","            next = np.zeros(self.num_nodes)\n","            for e in range(self.g.number_of_edges()):\n","                # print(new_influenced[self.g.edges()[0][e]])\n","                if((new_influenced[self.g.edges()[0][e]] == 1) and \n","                   not(tot_influenced[self.g.edges()[1][e]] == 1 or new_influenced[self.g.edges()[1][e]] == 1)):\n","                    r = random.random()\n","                    if(r < self.p):\n","                        # node influenced\n","                        next[self.g.edges()[1][e]] = 1\n","                        reward += 1\n","            # print(new_influenced)\n","            tot_influenced = tot_influenced + new_influenced\n","            new_influenced = next\n","        return reward\n","     \n","    def step(self, action):\n","        reward, sol, done = self._take_action(action)\n","        \n","        ob = self._build_ob()\n","        self.sol = sol\n","        info = {\"sol\": self.sol}\n","\n","        # need to convert ob to ndarray from tensor\n","        ob = ob.detach().cpu().numpy().ravel()\n","        next_state = np.copy(ob)\n","        return next_state, reward, done, info\n","    \n","    def _take_action(self, action):\n","        r1, r2 = 0, 0\n","        num_iter = 100\n","        for i in range(num_iter):\n","            r1 += self.compute_reward(self.x[:-1])\n","        if(self.x[action] == 0 and self.cost[action] <= self.max_budget):\n","            self.x[action] = 1\n","            self.x[-1] -= self.cost[action]\n","            self.max_budget -= self.cost[action]\n","        # write code for else case \n","        next_sol = 0\n","        for i in range(num_iter):\n","            r2 += self.compute_reward(self.x[:-1])\n","        done = self._check_done()\n","        return (r2 - r1)/num_iter, next_sol, done\n","\n","    def _check_done(self): \n","        inactive = (self.x[:-1] == 0).type(torch.float)\n","        # print(inactive)\n","        self.g.ndata['h'] = inactive\n","        not_selected = dgl.sum_nodes(self.g, 'h')\n","        self.g.ndata.pop('h')\n","        done = (not_selected == 0) or (self.max_budget <= 0)\n","        return done\n","                \n","    def _build_ob(self):\n","        ob_x = self.x\n","        # ob = torch.cat([ob_x], dim = 2)\n","        # return ob\n","        return ob_x\n","    \n","    # using num_samples = 1 as of now \n","    def register(self, g, num_samples = 1):\n","        self.g = g\n","        self.g.set_n_initializer(dgl.init.zero_initializer)\n","        t = torch.full((self.num_nodes, 1), 0, dtype=torch.float16)\n","        # torch.full(\n","            #     (self.num_nodes, 1),\n","            #     0, \n","            #     dtype = torch.long\n","            #     )\n","        self.x = torch.cat((t, torch.tensor([[self.max_budget]])), 0)\n","        ob = self._build_ob()\n","        return ob\n","\n","\n","    def reset(self):\n","        state = np.zeros(self.num_nodes + 1)\n","        state[-1] = self.BUDGET\n","\n","        t = torch.full((self.num_nodes, 1), 0, dtype=torch.float16)\n","        self.max_budget = self.BUDGET\n","        self.x = torch.cat((t, torch.tensor([[self.max_budget]])), 0)\n","\n","        return np.array(state)"],"metadata":{"id":"wLjmVoem_Lap","executionInfo":{"status":"ok","timestamp":1678872593542,"user_tz":-330,"elapsed":9,"user":{"displayName":"Sanchit Sinha","userId":"03766366414454501043"}}},"execution_count":259,"outputs":[]},{"cell_type":"markdown","source":["## Main "],"metadata":{"id":"vncLgXZQ_j18"}},{"cell_type":"code","source":["env = IM(maxb, 0.6, 5, cost)\n","ob = env.register(g)\n","\n","print(g.nodes())\n","print(g.edges())\n","print(ob)\n","# obnext, reward, done, info = env.step(3)\n","# print(obnext, reward, done, info)\n","# obnext, reward, done, info = env.step(3)\n","# print(obnext, reward, done, info)\n","# obnext, reward, done, info = env.step(3)\n","# print(obnext, reward, done, info)\n","\n","################################################################################"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"stmztkJwHoMy","executionInfo":{"status":"ok","timestamp":1678872593543,"user_tz":-330,"elapsed":10,"user":{"displayName":"Sanchit Sinha","userId":"03766366414454501043"}},"outputId":"bf59ff61-f4ee-4938-98ff-a76e3e027e92"},"execution_count":260,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0, 1, 2, 3, 4])\n","(tensor([0, 1, 2, 3, 0, 1, 2, 3, 4]), tensor([1, 2, 3, 4, 0, 1, 2, 3, 4]))\n","tensor([[   0.],\n","        [   0.],\n","        [   0.],\n","        [   0.],\n","        [   0.],\n","        [1000.]], dtype=torch.float16)\n"]}]},{"cell_type":"code","source":["# agent = Agent(state_size=env.num_nodes + 1, action_size=env.num_nodes, seed=0)\n","agent = Agent(state_size=env.num_nodes , action_size=env.num_nodes, seed=0)\n","\n","passed_state  = [1, 0, 0, 1, 0, 1000]\n","print(passed_state)\n","\n","node_state = passed_state[: -1 ]\n","print(node_state, node_state == 1)\n","\n","valid_idx = []\n","for i in range(len(node_state)):\n","    if(node_state[i] == 0):\n","        valid_idx.append(i)\n","\n","print(valid_idx)\n","\n","\n","print(random.choice(np.array(valid_idx)))\n"],"metadata":{"id":"r5tokBX_4C-t","executionInfo":{"status":"ok","timestamp":1678872593543,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sanchit Sinha","userId":"03766366414454501043"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"439d233a-1990-4ec0-b9aa-5cbcf021f306"},"execution_count":261,"outputs":[{"output_type":"stream","name":"stdout","text":["Graph(num_nodes=5, num_edges=9,\n","      ndata_schemes={'feat': Scheme(shape=(2,), dtype=torch.int64)}\n","      edata_schemes={}) tensor([[300,   0],\n","        [300,   0],\n","        [300,   0],\n","        [300,   0],\n","        [300,   0]])\n","[1, 0, 0, 1, 0, 1000]\n","[1, 0, 0, 1, 0] False\n","[1, 2, 4]\n","2\n"]}]},{"cell_type":"markdown","source":["### dqn"],"metadata":{"id":"tlTr-Dwc6lqk"}},{"cell_type":"code","source":["def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n","    \"\"\"Deep Q-Learning.\n","    \n","    Params\n","    ======\n","        n_episodes (int): maximum number of training episodes\n","        max_t (int): maximum number of timesteps per episode\n","        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n","        eps_end (float): minimum value of epsilon\n","        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n","    \"\"\"\n","    scores = []                        # list containing scores from each episode\n","    scores_window = deque(maxlen=100)  # last 100 scores\n","    eps = eps_start                    # initialize epsilon\n","    for i_episode in range(1, n_episodes+1):\n","        state = env.reset()\n","        score = 0\n","        for t in range(max_t):\n","            action = agent.act(env, state, eps)\n","            if action == -1:\n","                break \n","            next_state, reward, done, _ = env.step(action)\n","            agent.step(state, action, reward, next_state, done)\n","            state = next_state\n","            score += reward\n","            if done:\n","                break \n","\n","        scores_window.append(score)       # save most recent score\n","        scores.append(score)              # save most recent score\n","        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n","        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n","        if i_episode % 10 == 0:\n","            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n","        if np.mean(scores_window)>=200.0:\n","            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n","            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n","            break\n","    return scores\n","\n","scores = dqn(2)\n"],"metadata":{"id":"TceR8SAo7Gw6","executionInfo":{"status":"error","timestamp":1678872594191,"user_tz":-330,"elapsed":654,"user":{"displayName":"Sanchit Sinha","userId":"03766366414454501043"}},"colab":{"base_uri":"https://localhost:8080/","height":971},"outputId":"6b35a94c-0514-48ec-fbd8-00b387b3fc5a"},"execution_count":262,"outputs":[{"output_type":"stream","name":"stdout","text":["states tensor([[   0.,    0.,    0.,    1.,    0.,  700.],\n","        [   0.,    0.,    0.,    0.,    0., 1000.]])\n","actions tensor([[2],\n","        [3]])\n","rewards tensor([[-0.1000],\n","        [ 0.5900]])\n","next_states tensor([[  0.,   0.,   1.,   1.,   0., 400.],\n","        [  0.,   0.,   0.,   1.,   0., 700.]])\n","dones tensor([[0.],\n","        [0.]])\n","0 tensor([[300,   0],\n","        [300,   0],\n","        [300,   1],\n","        [300,   1],\n","        [300,   0]]) tensor([  0.,   0.,   1.,   1.,   0., 400.]) tensor([0., 0., 1., 1., 0.])\n","1 tensor([[300,   0],\n","        [300,   0],\n","        [300,   1],\n","        [300,   1],\n","        [300,   0]])\n","2 tensor([[29.3057]])\n","0 tensor([[300,   0],\n","        [300,   0],\n","        [300,   1],\n","        [300,   1],\n","        [300,   0]]) tensor([  0.,   0.,   0.,   1.,   0., 700.]) tensor([0., 0., 0., 1., 0.])\n","1 tensor([[300,   0],\n","        [300,   0],\n","        [300,   0],\n","        [300,   1],\n","        [300,   0]])\n","2 tensor([[29.2675]])\n","3 tensor([[43.3830,  6.0161, 45.6694, 56.2168, 16.8538]],\n","       grad_fn=<ReshapeAliasBackward0>)\n","3 tensor([[43.3910,  6.0015, 45.6714, 56.2108, 16.8564]],\n","       grad_fn=<ReshapeAliasBackward0>)\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-262-df825838cbba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-262-df825838cbba>\u001b[0m in \u001b[0;36mdqn\u001b[0;34m(n_episodes, max_t, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-258-3343c2bccb83>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-258-3343c2bccb83>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, experiences, gamma)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mQ_expected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_expected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0mQ_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got list)"]}]},{"cell_type":"code","source":["scores = dqn(100)\n"],"metadata":{"id":"RZ2ljAM1B3LQ","executionInfo":{"status":"aborted","timestamp":1678872594192,"user_tz":-330,"elapsed":6,"user":{"displayName":"Sanchit Sinha","userId":"03766366414454501043"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot the scores\n","fig = plt.figure()\n","ax = fig.add_subplot(111)\n","plt.plot(np.arange(len(scores)), scores)\n","plt.ylabel('Score')\n","plt.xlabel('Episode #')\n","plt.show()"],"metadata":{"id":"R6t_HlnH7zLo","executionInfo":{"status":"aborted","timestamp":1678872594192,"user_tz":-330,"elapsed":6,"user":{"displayName":"Sanchit Sinha","userId":"03766366414454501043"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xUi9IroyrME_","executionInfo":{"status":"aborted","timestamp":1678872594193,"user_tz":-330,"elapsed":7,"user":{"displayName":"Sanchit Sinha","userId":"03766366414454501043"}}},"execution_count":null,"outputs":[]}]}