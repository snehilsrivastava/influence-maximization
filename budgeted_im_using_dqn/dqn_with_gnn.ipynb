{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwzoA7pc7Ru1",
        "outputId": "463a617c-e317-4cd1-c482-708f723bcfeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dgl\n",
            "  Downloading dgl-1.0.1-cp39-cp39-manylinux1_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.9/dist-packages (from dgl) (1.22.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.9/dist-packages (from dgl) (5.9.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from dgl) (1.10.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.9/dist-packages (from dgl) (3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from dgl) (4.65.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from dgl) (2.27.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->dgl) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->dgl) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->dgl) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->dgl) (2.0.12)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install dgl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJyd7FY6_cFa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjNRn5fB_ctw"
      },
      "source": [
        "### Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJ4QvIIF7xbb",
        "outputId": "5262053d-e360-404d-c6a5-741433da155f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import dgl\n",
        "from collections import namedtuple\n",
        "import dgl.function as fn\n",
        "from copy import deepcopy as dc\n",
        "import random\n",
        "import time\n",
        "from time import time\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dgl import DGLGraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-nPNcXzHgmp"
      },
      "source": [
        "## Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3eW_PN7HfOy",
        "outputId": "5fc903fe-8ed6-4728-e5a2-8d1cd245b3e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'> 1 tensor([[300, 300, 300, 300, 300]])\n",
            "<class 'torch.Tensor'> 5 tensor([300, 300, 300, 300, 300])\n",
            "{'feat': tensor([[300,   0],\n",
            "        [300,   0],\n",
            "        [300,   0],\n",
            "        [300,   0],\n",
            "        [300,   0]])}\n",
            "tensor([0, 1, 2, 3, 4])\n",
            "(tensor([0, 1, 2, 3, 0, 1, 2, 3, 4]), tensor([1, 2, 3, 4, 0, 1, 2, 3, 4]))\n",
            "torch.Size([5, 2])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-f6e5f576eaf1>:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
            "  features = torch.tensor([cost.cpu().detach().numpy()])\n"
          ]
        }
      ],
      "source": [
        "cost = torch.tensor([300, 300, 300, 300, 300])\n",
        "\n",
        "features = torch.tensor([cost.cpu().detach().numpy()])\n",
        "print(type(features), len(features), features)\n",
        "print(type(features[0]), len(features[0]), features[0])\n",
        "\n",
        "maxb = 1000\n",
        "src_ids = torch.tensor([0, 1, 2, 3])\n",
        "dst_ids = torch.tensor([1, 2, 3, 4])\n",
        "\n",
        "g = dgl.graph((src_ids, dst_ids), num_nodes=5)\n",
        "g = dgl.add_self_loop(g)\n",
        "g.ndata['feat'] = torch.tensor([[300, 0],[300, 0],[300, 0],[300, 0],[300, 0]])\n",
        "\n",
        "print(g.ndata)\n",
        "print(g.nodes())\n",
        "print(g.edges())\n",
        "print(g.ndata['feat'].shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYWBA7fY3KyE"
      },
      "source": [
        "## Model\n",
        "https://discuss.dgl.ai/t/node-regression-in-heterograph/2492/6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3zPG8vXUWhQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "class RGCN(nn.Module):\n",
        "    def __init__(self, in_feats, h_feats, num_classes, seed):\n",
        "        super(RGCN, self).__init__()\n",
        "        self.seed = seed\n",
        "        self.conv1 = GraphConv(in_feats, h_feats)\n",
        "        self.conv2 = GraphConv(h_feats, h_feats)\n",
        "        self.fc3 = nn.Linear(h_feats, num_classes)\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, g, in_feat):\n",
        "        inp = len(in_feat) \n",
        "        print(\"0\", inp)\n",
        "        h = self.conv1(g, in_feat)\n",
        "        print(\"1\", h, h.shape)\n",
        "        h = F.relu(h)\n",
        "        print(\"2\", h, h.shape)\n",
        "        h = self.conv2(g, h)\n",
        "        print(\"3\", h, h.shape)\n",
        "        h = self.fc3(F.relu(h))\n",
        "        print(\"4\", h, h.shape)\n",
        "        T = h.mean(dim=0)\n",
        "        T = T.mean(dim=0)\n",
        "        return T.reshape(inp, self.num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "_p0sZJZN4xbq",
        "outputId": "6dd2560a-6294-4056-cece-50264cfaf0fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[300,   1],\n",
            "         [300,   1],\n",
            "         [300,   0],\n",
            "         [300,   0],\n",
            "         [300,   0]],\n",
            "\n",
            "        [[300,   1],\n",
            "         [300,   1],\n",
            "         [300,   0],\n",
            "         [300,   0],\n",
            "         [300,   0]]]) torch.Size([2, 5, 2])\n",
            "0 2\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-df1822bde356>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# print(st2, f2, f2.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRGCN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-11c902ffb589>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, g, in_feat)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/dgl/nn/pytorch/conv/graphconv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, graph, feat, weight, edge_weight)\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0mshp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfeat_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m                 \u001b[0mfeat_src\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeat_src\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (5) at non-singleton dimension 0"
          ]
        }
      ],
      "source": [
        "# model = RGCN(g.ndata['feat'].shape[1], 16, 5, 0)\n",
        "\n",
        "st1 = torch.tensor([1, 0, 0, 0, 0])\n",
        "st2 = torch.tensor([1, 1, 0, 0, 0])\n",
        "\n",
        "f1 = g.ndata['feat']\n",
        "f1[:, 1] = st1\n",
        "\n",
        "f2 = g.ndata['feat']\n",
        "f2[:, 1] = st2\n",
        "\n",
        "# model = RGCN(f1.shape[1], 16, 5, 0)\n",
        "# print(st1, f1, f1.shape)\n",
        "# action = model(g, f1)\n",
        "# print(action)\n",
        "\n",
        "# # f = torch.tensor()\n",
        "\n",
        "\n",
        "f2 = torch.tensor([[[300,   1],\n",
        "        [300,   1],\n",
        "        [300,   0],\n",
        "        [300,   0],\n",
        "        [300,   0]]])\n",
        "\n",
        "f1 = np.array([[300,   1],\n",
        "        [300,   1],\n",
        "        [300,   0],\n",
        "        [300,   0],\n",
        "        [300,   0]])\n",
        "f3 = torch.tensor([f1 ,f1])\n",
        "print(f3, f3.shape)\n",
        "# print(st2, f2, f2.shape)\n",
        "model = RGCN(f3.shape[2], 16, 5, 0)\n",
        "action = model(g, f3)\n",
        "print(action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad16n6GO3PG7"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3b4yeDgoNak"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
        "# BATCH_SIZE = 64         # minibatch size\n",
        "BATCH_SIZE = 2         # minibatch size\n",
        "GAMMA = 0.99            # discount factor\n",
        "TAU = 1e-3              # for soft update of target parameters\n",
        "LR = 5e-4               # learning rate \n",
        "# UPDATE_EVERY = 4        # how often to update the network\n",
        "UPDATE_EVERY = 1       # how often to update the network\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Agent():\n",
        "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed):\n",
        "        \"\"\"Initialize an Agent object.\n",
        "        \n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): dimension of each state\n",
        "            action_size (int): dimension of each action\n",
        "            seed (int): random seed\n",
        "        \"\"\"\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        print(g, features)\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "        # Q-Network\n",
        "        # model = RGCN(g.ndata['feat'].shape[1], 16, dataset.num_classes)\n",
        "        self.qnetwork_local = RGCN(g.ndata['feat'].shape[1], 16, action_size, seed).to(device)\n",
        "        self.qnetwork_target = RGCN(g.ndata['feat'].shape[1], 16, action_size, seed).to(device)\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
        "\n",
        "        # Replay memory\n",
        "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
        "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
        "        self.t_step = 0\n",
        "    \n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        # Save experience in replay memory\n",
        "        # print(\"Add to memory : \", state, action, next_state, reward)\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "        \n",
        "        # Learn every UPDATE_EVERY time steps.\n",
        "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
        "        if self.t_step == 0:\n",
        "            # If enough samples are available in memory, get random subset and learn\n",
        "            if len(self.memory) > BATCH_SIZE:\n",
        "                experiences = self.memory.sample()\n",
        "                self.learn(experiences, GAMMA)\n",
        "\n",
        "    def act(self, env, state, eps=0.):\n",
        "        \"\"\"Returns actions for given state as per current policy.\n",
        "        \n",
        "        Params\n",
        "        ======\n",
        "            state (array_like): current state\n",
        "            eps (float): epsilon, for epsilon-greedy action selection\n",
        "        \"\"\"\n",
        "        node_state = state[: -1 ]\n",
        "        valid_action = []\n",
        "        for i in range(len(node_state)):\n",
        "            if(node_state[i] == 0 and env.cost[i] <= env.max_budget):\n",
        "                valid_action.append(True)\n",
        "            else:\n",
        "                valid_action.append(False)\n",
        "\n",
        "        valid_action_arg = []\n",
        "        for i in range(len(valid_action)):\n",
        "            if valid_action[i] == True:\n",
        "                valid_action_arg.append(i)\n",
        "\n",
        "\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        self.qnetwork_local.eval()\n",
        "        with torch.no_grad():\n",
        "            new_features = g.ndata['feat']\n",
        "            # print(state, self.state_size,  state[:,: self.state_size])\n",
        "            new_features[:, 1] = state[:,: self.state_size]\n",
        "            action_values = self.qnetwork_local(g, new_features)\n",
        "            # print(\"training local model\", state, action_values)\n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "        mn = -1e9\n",
        "        for i in range(len(valid_action)):\n",
        "            if(valid_action[i] == False):\n",
        "                action_values[0][i] = mn\n",
        "\n",
        "        # print(\"action_values\", action_values)\n",
        "        # print(\"valid_idx\", valid_action_arg)\n",
        "        \n",
        "        # Epsilon-greedy action selection\n",
        "        if random.random() > eps:\n",
        "            # picks action with maximum value attached to it \n",
        "            choice = np.argmax(action_values.cpu().data.numpy())\n",
        "            if choice == mn : \n",
        "                return -1\n",
        "\n",
        "            # print(\"non greedy : \", choice)\n",
        "            return choice\n",
        "        else:\n",
        "            # randomly pick any action from the values that are not already chosen \n",
        "            # return random.choice(np.arange(self.action_size))\n",
        "            if len(valid_action_arg) == 0 :\n",
        "                return -1\n",
        "                \n",
        "            choice = random.choice(valid_action_arg)\n",
        "            # print(\"greedy : \", choice)\n",
        "            return choice\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        \"\"\"Update value parameters using given batch of experience tuples.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
        "            gamma (float): discount factor\n",
        "        \"\"\"\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "        print(\"states\", states)\n",
        "        print(\"actions\", actions)\n",
        "        print(\"rewards\", rewards)\n",
        "        print(\"next_states\", next_states)\n",
        "        print(\"dones\", dones)\n",
        "              \n",
        "        # Q_targets_next = self.qnetwork_target(g, features).detach().max(1)[0].unsqueeze(1)\n",
        "        # # Compute Q targets for current states \n",
        "        # Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "        Q_targets = []\n",
        "        for i in range(len(next_states)) :\n",
        "            ns = next_states[i]\n",
        "            f = g.ndata['feat']\n",
        "            print(\"0\", f, ns, ns[: self.state_size])\n",
        "            f[:, 1] = ns[: self.state_size]\n",
        "            print(\"1\", f)\n",
        "            q_target_next = self.qnetwork_target(g, f).detach().max(1)[0].unsqueeze(1)\n",
        "            print(\"2\", q_target_next)\n",
        "            q_target = rewards[i] + (gamma * q_target_next * (1 - dones[i]))\n",
        "            Q_targets.append(q_target)\n",
        "\n",
        "        # Q_expected = self.qnetwork_local(g, features).gather(1, actions)\n",
        "        Q_expected = []\n",
        "        for i in range(len(states)):\n",
        "            s = states[i]\n",
        "            f = g.ndata['feat']\n",
        "            f[:, 1] = s[: self.state_size]\n",
        "            action_values = self.qnetwork_local(g, f)\n",
        "            print(\"3\", action_values)\n",
        "            choice = np.argmax(action_values.cpu().data.numpy())\n",
        "            q_expected = action_values[:, choice]\n",
        "            Q_expected.append(q_expected)\n",
        "\n",
        "\n",
        "        Q_expected = np.array(Q_expected)\n",
        "        Q_targets = np.array(Q_targets)\n",
        "\n",
        "        Q_expected = torch.from_numpy(Q_expected)\n",
        "        Q_targets = torch.from_numpy(Q_targets)\n",
        "        \n",
        "        # Compute loss\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "        # Minimize the loss\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # ------------------- update target network ------------------- #\n",
        "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)                     \n",
        "\n",
        "    def soft_update(self, local_model, target_model, tau):\n",
        "        \"\"\"Soft update model parameters.\n",
        "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            local_model (PyTorch model): weights will be copied from\n",
        "            target_model (PyTorch model): weights will be copied to\n",
        "            tau (float): interpolation parameter \n",
        "        \"\"\"\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        \"\"\"Initialize a ReplayBuffer object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            action_size (int): dimension of each action\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            batch_size (int): size of each training batch\n",
        "            seed (int): random seed\n",
        "        \"\"\"\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)  \n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "    \n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "    \n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "  \n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU9M6sGe_L-3"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLjmVoem_Lap"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import dgl\n",
        "from collections import namedtuple\n",
        "import dgl.function as fn\n",
        "from copy import deepcopy as dc\n",
        "import random\n",
        "import time\n",
        "from time import time\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "class IM(object):\n",
        "    def __init__(self, max_budget, p, num_nodes, cost):\n",
        "        self.max_budget = max_budget\n",
        "        self.BUDGET = max_budget\n",
        "        assert(p <= 1 and p >= 0)\n",
        "        self.p = p\n",
        "        self.num_nodes = num_nodes\n",
        "        self.cost = cost\n",
        "\n",
        "    def compute_reward(self, state):\n",
        "        # reward is the number of additional nodes influenced\n",
        "        reward = 0\n",
        "        # each node has one chance to influence each neighbour\n",
        "        new_influenced = state.detach().cpu().numpy().ravel()\n",
        "        tot_influenced = state.detach().cpu().numpy().ravel()\n",
        "        while((new_influenced == 1).sum() >= 1):\n",
        "            # next = torch.full(\n",
        "            #     (self.num_nodes, 1),\n",
        "            #     0, \n",
        "            #     dtype = torch.long\n",
        "            #     )\n",
        "            next = np.zeros(self.num_nodes)\n",
        "            for e in range(self.g.number_of_edges()):\n",
        "                # print(new_influenced[self.g.edges()[0][e]])\n",
        "                if((new_influenced[self.g.edges()[0][e]] == 1) and \n",
        "                   not(tot_influenced[self.g.edges()[1][e]] == 1 or new_influenced[self.g.edges()[1][e]] == 1)):\n",
        "                    r = random.random()\n",
        "                    if(r < self.p):\n",
        "                        # node influenced\n",
        "                        next[self.g.edges()[1][e]] = 1\n",
        "                        reward += 1\n",
        "            # print(new_influenced)\n",
        "            tot_influenced = tot_influenced + new_influenced\n",
        "            new_influenced = next\n",
        "        return reward\n",
        "     \n",
        "    def step(self, action):\n",
        "        reward, sol, done = self._take_action(action)\n",
        "        \n",
        "        ob = self._build_ob()\n",
        "        self.sol = sol\n",
        "        info = {\"sol\": self.sol}\n",
        "\n",
        "        # need to convert ob to ndarray from tensor\n",
        "        ob = ob.detach().cpu().numpy().ravel()\n",
        "        next_state = np.copy(ob)\n",
        "        return next_state, reward, done, info\n",
        "    \n",
        "    def _take_action(self, action):\n",
        "        r1, r2 = 0, 0\n",
        "        num_iter = 100\n",
        "        for i in range(num_iter):\n",
        "            r1 += self.compute_reward(self.x[:-1])\n",
        "        if(self.x[action] == 0 and self.cost[action] <= self.max_budget):\n",
        "            self.x[action] = 1\n",
        "            self.x[-1] -= self.cost[action]\n",
        "            self.max_budget -= self.cost[action]\n",
        "        # write code for else case \n",
        "        next_sol = 0\n",
        "        for i in range(num_iter):\n",
        "            r2 += self.compute_reward(self.x[:-1])\n",
        "        done = self._check_done()\n",
        "        return (r2 - r1)/num_iter, next_sol, done\n",
        "\n",
        "    def _check_done(self): \n",
        "        inactive = (self.x[:-1] == 0).type(torch.float)\n",
        "        # print(inactive)\n",
        "        self.g.ndata['h'] = inactive\n",
        "        not_selected = dgl.sum_nodes(self.g, 'h')\n",
        "        self.g.ndata.pop('h')\n",
        "        done = (not_selected == 0) or (self.max_budget <= 0)\n",
        "        return done\n",
        "                \n",
        "    def _build_ob(self):\n",
        "        ob_x = self.x\n",
        "        # ob = torch.cat([ob_x], dim = 2)\n",
        "        # return ob\n",
        "        return ob_x\n",
        "    \n",
        "    # using num_samples = 1 as of now \n",
        "    def register(self, g, num_samples = 1):\n",
        "        self.g = g\n",
        "        self.g.set_n_initializer(dgl.init.zero_initializer)\n",
        "        t = torch.full((self.num_nodes, 1), 0, dtype=torch.float16)\n",
        "        # torch.full(\n",
        "            #     (self.num_nodes, 1),\n",
        "            #     0, \n",
        "            #     dtype = torch.long\n",
        "            #     )\n",
        "        self.x = torch.cat((t, torch.tensor([[self.max_budget]])), 0)\n",
        "        ob = self._build_ob()\n",
        "        return ob\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        state = np.zeros(self.num_nodes + 1)\n",
        "        state[-1] = self.BUDGET\n",
        "\n",
        "        t = torch.full((self.num_nodes, 1), 0, dtype=torch.float16)\n",
        "        self.max_budget = self.BUDGET\n",
        "        self.x = torch.cat((t, torch.tensor([[self.max_budget]])), 0)\n",
        "\n",
        "        return np.array(state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vncLgXZQ_j18"
      },
      "source": [
        "## Main "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stmztkJwHoMy",
        "outputId": "ee4f56bf-ceb7-4339-a0dd-31f943b1b7e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0, 1, 2, 3, 4])\n",
            "(tensor([0, 1, 2, 3, 0, 1, 2, 3, 4]), tensor([1, 2, 3, 4, 0, 1, 2, 3, 4]))\n",
            "tensor([[   0.],\n",
            "        [   0.],\n",
            "        [   0.],\n",
            "        [   0.],\n",
            "        [   0.],\n",
            "        [1000.]], dtype=torch.float16)\n"
          ]
        }
      ],
      "source": [
        "env = IM(maxb, 0.6, 5, cost)\n",
        "ob = env.register(g)\n",
        "\n",
        "print(g.nodes())\n",
        "print(g.edges())\n",
        "print(ob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5tokBX_4C-t"
      },
      "outputs": [],
      "source": [
        "# agent = Agent(state_size=env.num_nodes + 1, action_size=env.num_nodes, seed=0)\n",
        "agent = Agent(state_size=env.num_nodes , action_size=env.num_nodes, seed=0)\n",
        "\n",
        "passed_state  = [1, 0, 0, 1, 0, 1000]\n",
        "print(passed_state)\n",
        "\n",
        "node_state = passed_state[: -1 ]\n",
        "print(node_state, node_state == 1)\n",
        "\n",
        "valid_idx = []\n",
        "for i in range(len(node_state)):\n",
        "    if(node_state[i] == 0):\n",
        "        valid_idx.append(i)\n",
        "\n",
        "print(valid_idx)\n",
        "\n",
        "\n",
        "print(random.choice(np.array(valid_idx)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlTr-Dwc6lqk"
      },
      "source": [
        "### dqn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TceR8SAo7Gw6"
      },
      "outputs": [],
      "source": [
        "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
        "    \"\"\"Deep Q-Learning.\n",
        "    \n",
        "    Params\n",
        "    ======\n",
        "        n_episodes (int): maximum number of training episodes\n",
        "        max_t (int): maximum number of timesteps per episode\n",
        "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
        "        eps_end (float): minimum value of epsilon\n",
        "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
        "    \"\"\"\n",
        "    scores = []                        # list containing scores from each episode\n",
        "    scores_window = deque(maxlen=100)  # last 100 scores\n",
        "    eps = eps_start                    # initialize epsilon\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            action = agent.act(env, state, eps)\n",
        "            if action == -1:\n",
        "                break \n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                break \n",
        "\n",
        "        scores_window.append(score)       # save most recent score\n",
        "        scores.append(score)              # save most recent score\n",
        "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
        "        if i_episode % 10 == 0:\n",
        "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "        if np.mean(scores_window)>=200.0:\n",
        "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
        "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
        "            break\n",
        "    return scores\n",
        "\n",
        "scores = dqn(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZ2ljAM1B3LQ"
      },
      "outputs": [],
      "source": [
        "scores = dqn(100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6t_HlnH7zLo"
      },
      "outputs": [],
      "source": [
        "# plot the scores\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "plt.plot(np.arange(len(scores)), scores)\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Episode #')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUi9IroyrME_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
